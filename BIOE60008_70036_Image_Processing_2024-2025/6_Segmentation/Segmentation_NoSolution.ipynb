{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<div align='center'><div align='center'>\n",
    "\n",
    "## Image Processing\n",
    "\n",
    "### Image Segmentation\n",
    "\n",
    "#### Version 1.0, 12th Dec, 2024\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This notebook focuses on segmenting lung regions affected by COVID-19 in computed tomography (CT) scans. Segmentation highlights critical features such as ground-glass opacities and consolidations, which are vital for diagnosing COVID-19. Accurate identification of these regions is essential for assessing the severity of infection and guiding treatment plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this tutorial, we will:\n",
    "1. Explore the COVID-19 CT dataset.\n",
    "2. Preprocess the data for segmentation tasks.\n",
    "3. Build a segmentation model using a U-Net architecture.\n",
    "4. Train the model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Step 1: Explore dataset\n",
    "\n",
    "The dataset used in this tutorial consists of 2D lung CT scans along with their corresponding segmentation masks. It includes 100 axial CT images and the corresponding annotated masks (0 - \"ground glass\", 1 - \"consolidations\", 2 - \"lungs other\", 3 - \"background\"), derived from over 40 COVID-19 patients, originally sourced from publicly available JPG images and subsequently converted for this study.\n",
    "\n",
    "Corresponding segmentation masks contain four channels:\n",
    "\n",
    "1) Ground glass opacities (GGO)\n",
    "2) Consolidations\n",
    "3) Normal lung tissues\n",
    "4) Background\n",
    "\n",
    "Each image has dimensions of 520 Ã— 520 pixels. The segmentation masks identify regions of interest within the lungs, including infected areas (such as ground-glass opacities and consolidations) and normal lung tissue. Here we start from exploring the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to concatenate file chunks\n",
    "def concatenate_files(output_file, chunk_prefix):\n",
    "    \"\"\"\n",
    "    Concatenates all file chunks with a given prefix into a single output file.\n",
    "\n",
    "    Parameters:\n",
    "    - output_file (str): The name of the output file to create.\n",
    "    - chunk_prefix (str): The common prefix of all the file chunks to merge.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'wb') as outfile:\n",
    "        # Get all chunk files starting with the prefix and sort them\n",
    "        chunk_files = sorted([f for f in os.listdir('.') if f.startswith(chunk_prefix)])\n",
    "        \n",
    "        for chunk_file in chunk_files:\n",
    "            with open(chunk_file, 'rb') as infile:\n",
    "                outfile.write(infile.read())\n",
    "            print(f\"Added {chunk_file} to {output_file}\")\n",
    "    print(f\"File {output_file} created successfully!\")\n",
    "\n",
    "# Concatenate the chunks for images_medseg.npy\n",
    "concatenate_files('images_medseg.npy', 'images_part_')\n",
    "\n",
    "# Concatenate the chunks for masks_medseg.npy\n",
    "concatenate_files('masks_medseg.npy', 'masks_part_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T11:48:14.348031Z",
     "start_time": "2024-12-12T11:48:14.235366Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "data_path = './'     # this could be changed if you put the training data to other places\n",
    "images_medseg = np.load(os.path.join(data_path, 'images_medseg.npy'))\n",
    "masks_medseg = np.load(os.path.join(data_path, 'masks_medseg.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Question-1:** What are the types of the images and masks? You may use print(variable.dtype) to get the type of the variables.\n",
    "Generally, which type do we need for training? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:19:50.110060Z",
     "start_time": "2024-12-12T12:19:50.106561Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(image_batch, mask_batch=None, pred_batch=None, num_samples=8, hot_encode=True):\n",
    "    num_classes = mask_batch.shape[-1] if mask_batch is not None else 0\n",
    "    fix, ax = plt.subplots(num_classes + 1, num_samples, figsize=(num_samples * 2, (num_classes + 1) * 2))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        ax_image = ax[0, i] if num_classes > 0 else ax[i]\n",
    "        if hot_encode:\n",
    "            ax_image.imshow(image_batch[i,:,:,0], cmap='Greys')\n",
    "        else:\n",
    "            ax_image.imshow(image_batch[i,:,:])\n",
    "        ax_image.set_xticks([])\n",
    "        ax_image.set_yticks([])\n",
    "\n",
    "        if mask_batch is not None:\n",
    "            for j in range(num_classes):\n",
    "                if pred_batch is None:\n",
    "                    mask_to_show = mask_batch[i,:,:,j]\n",
    "                else:\n",
    "                    mask_to_show = np.zeros(shape=(*mask_batch.shape[1:-1], 3))\n",
    "                    mask_to_show[..., 0] = pred_batch[i,:,:,j].cpu().numpy() > 0.5\n",
    "                    mask_to_show[..., 1] = mask_batch[i,:,:,j]\n",
    "                ax[j + 1, i].imshow(mask_to_show, vmin=0, vmax=1)\n",
    "                ax[j + 1, i].set_xticks([])\n",
    "                ax[j + 1, i].set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:19:52.462052Z",
     "start_time": "2024-12-12T12:19:51.450732Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "visualize(images_medseg[10:], masks_medseg[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Step 2: Data preprocessing\n",
    "\n",
    "**Question-2:** What is the range of a normal RGB/Grey image? What is the range for a CT scan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Answer-2:** The range for RGB/Grey images is [0,255].\n",
    "However, for CT scans, it is different. In CT images, we use the Hounsfield Unit (HU) as the standardized measure to quantify the density of tissues. It is derived from the linear attenuation coefficient of tissues compared to water, where water is assigned a value of 0 HU and air is -1000 HU. Dense structures like bone have high positive values (e.g., +1000 HU), while less dense materials like fat have lower values (e.g., -100 HU).\n",
    "The HU scale enables consistent interpretation of tissue densities across CT scans, making it crucial for medical diagnosis and research. The range for a human CT scan is approximately from -1000 to 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Use the following function to visualize the histogram of the 10th image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:34:58.932521Z",
     "start_time": "2024-12-12T14:34:58.626170Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def visualize_histogram(image):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(image.flatten(), bins=100, color='blue', alpha=0.7)\n",
    "    plt.title('CT Image Histogram')\n",
    "    plt.xlabel('Pixel Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "visualize_histogram(images_medseg[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Window shift and normalization\n",
    "From the histogram above, you will find most pixels range between [-1000, -700] and [-200,200]. In computed tomography (CT), a window transform refers to the process of adjusting the display settings to focus on specific ranges of Hounsfield Units (HU), which correspond to different tissue densities. This adjustment enhances the visibility of certain tissues or structures in the CT images by mapping the HU range to the grayscale display.\n",
    "\n",
    "**Key Concepts of Window Transform**\n",
    "\n",
    "1) Window Width (WW): Determines the range of HU values displayed in the image. Pixels with HU values outside this range are set to the minimum (black) or maximum (white) grayscale values. A narrow window width enhances contrast but may lose detail in areas outside the range.\n",
    "2) Window Level (WL): Specifies the center of the HU range (or window). Adjusting the window level shifts the range of displayed HU values, allowing focus on different tissue types.\n",
    "\n",
    "**Common Window Settings in CT**\n",
    "Lung: WW ~ 1500 HU, WL ~ -600 HU.\n",
    "Bone: WW ~ 2000 HU, WL ~ 500 HU.\n",
    "Soft Tissue Window: WW ~ 400 HU, WL ~ 40 HU.\n",
    "\n",
    "**Question-3**: complete the following code for window shift with WW ~ 1500 HU, WL ~ -600 HU, and rescale the image to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:52:16.155672Z",
     "start_time": "2024-12-12T15:52:16.141164Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def window_shift_and_normalize(ct_image, window_center, window_width):\n",
    "    \"\"\"\n",
    "    Apply window shift and normalization to a CT image.\n",
    "\n",
    "    Parameters:\n",
    "    - ct_image (numpy array): The input CT image (HU values).\n",
    "    - window_center (int): The center of the window (e.g., 40 for soft tissue).\n",
    "    - window_width (int): The width of the window (e.g., 400 for soft tissue).\n",
    "\n",
    "    Returns:\n",
    "    - normalized_image (numpy array): The processed image with values normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the window bounds (replace the placeholders)\n",
    "    window_min = ... # Lower bound of the window\n",
    "    window_max = ... # Upper bound of the window\n",
    "\n",
    "    # Step 2: Clip the image to the window range (replace the placeholder)\n",
    "    shifted_image = np.clip(..., ..., ...)\n",
    "\n",
    "    # Step 3: Normalize the image to [0, 1] (replace the placeholder)\n",
    "    normalized_image = (... - ...) / (... - ...)\n",
    "\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You should get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:50:09.675951Z",
     "start_time": "2024-12-12T15:50:09.565305Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_10th, masks_10th = images_medseg[10], masks_medseg[10]\n",
    "image_trans = window_shift_and_normalize(image_10th,-600, 1500)\n",
    "plt.figure()\n",
    "plt.imshow(image_trans*255,cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Visualize segmentation mask\n",
    "Here you need to merge the one-hot mask to the segmentation mask for visualization to make sure everything goes right.\n",
    "\n",
    "**Question-4:** How to convert the onehot mask to the segmentation mask?\n",
    "\n",
    "```\n",
    "seg_mask10 =\n",
    "plt.figure()\n",
    "plt.imshow(seg_mask10)\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You should get something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:51:04.178625Z",
     "start_time": "2024-12-12T15:51:04.100065Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seg_mask10 =\n",
    "plt.figure()\n",
    "plt.imshow()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Step 3: Build your Dataloader\n",
    "Now it's time to build your own Pytorch dataloader. The DataLoader in PyTorch is a utility that provides an efficient way to load and preprocess data for training or inference in deep learning models. It works by combining a dataset and a sampler, allowing you to easily iterate over data in batches, shuffle data, and use multiprocessing to speed up data loading.\n",
    "\n",
    "**Question-5:**\n",
    "Build your own Pytorch dataloader to load the data based on the following example:\n",
    "```python\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class Covid19_Dataloader(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return image, mask\n",
    "```\n",
    "\n",
    "Please note that you need to\n",
    "1. add window adjusting function into the class 'Covid19_Dataloader' for window shifting\n",
    "2. add randomflip for data augmentation and set the flipping prob to 0.45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Step 4: Build your neural network\n",
    "Now, you are ready to build a neural network for training. Here we will use a simple UNet as an example.\n",
    "You are encouraged to build your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:36:30.872671Z",
     "start_time": "2024-12-12T20:36:30.867173Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "class Covid19_Dataloader(Dataset):\n",
    "    def __init__(self, images, masks, flip_prob=0.5):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.flip_prob = flip_prob\n",
    "        self.to_Tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def window_shift_and_normalize(self, ct_image, window_center, window_width):\n",
    "        window_min = window_center - (window_width / 2)\n",
    "        window_max = window_center + (window_width / 2)\n",
    "\n",
    "        shifted_image = np.clip(ct_image, window_min, window_max)\n",
    "\n",
    "        normalized_image = (shifted_image - window_min) / (window_max - window_min)\n",
    "\n",
    "        return normalized_image\n",
    "\n",
    "    def random_flip(self, image, mask):\n",
    "        \"\"\"Randomly flip both image and mask horizontally and/or vertically.\"\"\"\n",
    "        if random.random() < self.flip_prob:\n",
    "            # Horizontal flip\n",
    "            image = torch.flip(image, dims=[2])\n",
    "            mask = torch.flip(mask, dims=[2])\n",
    "        if random.random() < self.flip_prob:\n",
    "            # Vertical flip\n",
    "            image = torch.flip(image, dims=[1])\n",
    "            mask = torch.flip(mask, dims=[1])\n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = self.window_shift_and_normalize(image, window_center=-600, window_width=1500)\n",
    "        mask = self.masks[idx]\n",
    "        image_T, mask_T = self.to_Tensor(image.astype(np.float32)), self.to_Tensor(mask.astype(np.float32))\n",
    "        image, mask = self.random_flip(image_T, mask_T)\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**FCN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FCN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**UNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.conv1 = DoubleConv(in_ch, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = DoubleConv(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = DoubleConv(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = DoubleConv(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.conv5 = DoubleConv(256, 512)\n",
    "        self.up6 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.conv6 = DoubleConv(512, 256)\n",
    "        self.up7 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv7 = DoubleConv(256, 128)\n",
    "        self.up8 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv8 = DoubleConv(128, 64)\n",
    "        self.up9 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.conv9 = DoubleConv(64, 32)\n",
    "        self.conv10 = nn.Conv2d(32, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "        #print(p1.shape)\n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "        #print(p2.shape)\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "        #print(p3.shape)\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "        #print(p4.shape)\n",
    "        c5 = self.conv5(p4)\n",
    "        up_6 = self.up6(c5)\n",
    "        merge6 = torch.cat([up_6, c4], dim=1)\n",
    "        c6 = self.conv6(merge6)\n",
    "        up_7 = self.up7(c6)\n",
    "        merge7 = torch.cat([up_7, c3], dim=1)\n",
    "        c7 = self.conv7(merge7)\n",
    "        up_8 = self.up8(c7)\n",
    "        merge8 = torch.cat([up_8, c2], dim=1)\n",
    "        c8 = self.conv8(merge8)\n",
    "        up_9 = self.up9(c8)\n",
    "        merge9 = torch.cat([up_9, c1], dim=1)\n",
    "        c9 = self.conv9(merge9)\n",
    "        c10 = self.conv10(c9)\n",
    "        out = nn.Sigmoid()(c10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### the Attention-UNet\n",
    "![jupyter](./attention.png)\n",
    "Schematic of the attention gate (AG). Input features $x^l$ are scaled with attention coefficients ($\\alpha$) computed in AG. Spatial regions are selected by analysing both the activations and contextual information provided by the gating signal ($g$) which is collected from a coarser scale. Grid resampling of attention coefficients is done using trilinear interpolation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The attention gate is formulated as:\n",
    "$$\n",
    "q_{\\text{att}}^l = \\psi^T \\left( \\sigma_1 \\left( W_x^T x_i^l + W_g^T g_i + b_g \\right) \\right) + b_\\psi,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_i^l = \\sigma_2 \\left( q_{\\text{att}}^l \\left( x_i^l, g_i; \\Theta_{\\text{att}} \\right) \\right),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\sigma_2(x_{i,c}) = \\frac{1}{1 + \\exp(-x_{i,c})}\n",
    "$$\n",
    "\n",
    "This corresponds to the sigmoid activation function. AG is characterized by a set of parameters $\\Theta_{\\text{att}}$ containing: linear transformations: $W_x \\in \\mathbb{R}^{F_l \\times F_{\\text{int}}}$, $W_g \\in \\mathbb{R}^{F_g \\times F_{\\text{int}}}$, $\\psi \\in \\mathbb{R}^{F_{\\text{int}} \\times 1}$ and bias terms $b_\\psi \\in \\mathbb{R}$, $b_g \\in \\mathbb{R}^{F_{\\text{int}}}$. The linear transformations are computed using channel-wise $1 \\times 1 \\times 1$ convolutions for the input tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question-6**: write the attention block\n",
    "```python\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # 1x1 conv for signal g\n",
    "        g1 =\n",
    "        # 1x1 conv for signal x^l\n",
    "        x1 =\n",
    "        # concat + relu\n",
    "        psi =\n",
    "        # get attention map\n",
    "        psi =\n",
    "        return x * psi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can get the attention UNet based on the attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=1):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64, ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128, ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256, ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512, ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
    "        self.Att5 = Attention_block(F_g=512, F_l=512, F_int=256)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
    "        self.Att4 = Attention_block(F_g=256, F_l=256, F_int=128)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
    "        self.Att3 = Attention_block(F_g=128, F_l=128, F_int=64)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
    "        self.Att2 = Attention_block(F_g=64, F_l=64, F_int=32)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5, x=x4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=x3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=x2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=x1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        d1 = self.sigmoid(d1)\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Training\n",
    "Now you have your dataloader ready, your network ready, and it's time to train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:14:25.785808Z",
     "start_time": "2024-12-12T21:14:25.676082Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = UNet()\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_medseg, masks_medseg, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Covid19_Dataloader(X_train, y_train, flip_prob=0.45)\n",
    "val_dataset = Covid19_Dataloader(X_val, y_val,  flip_prob=0.45)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:14:57.060427Z",
     "start_time": "2024-12-12T21:14:26.283711Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    average_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(average_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, marker='o', linestyle='-')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Step 5: Evaluation\n",
    "Now we have completed the training part. However, we don't know the model's performance in addition to the training losses.\n",
    "\n",
    "Intersection over Union (IoU), also known as the Jaccard Index, is a metric commonly used to evaluate the performance of image segmentation models. It measures the overlap between the predicted segmentation mask and the ground truth mask.\n",
    "IoU is particularly useful in assessing how well a model predicts regions of interest, such as objects or tissues, in an image.\n",
    "\n",
    "**IoU Calculation Formula**\n",
    "The IoU is calculated as the ratio of the intersection to the union of the predicted and ground truth masks:\n",
    "\n",
    "$$\n",
    "IoU = \\frac{\\text{Intersection}}{\\text{Union}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Intersection**: The area of overlap between the predicted mask and the ground truth mask.\n",
    "- **Union**: The total area covered by the predicted mask and the ground truth mask (including the overlap).\n",
    "\n",
    "This can also be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "IoU = \\frac{|P \\cap T|}{|P \\cup T|} = \\frac{|P \\cap T|}{|P| + |T| - |P \\cap T|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(P\\): Predicted mask.\n",
    "- \\(T\\): Ground truth mask.\n",
    "- $|P \\cap T|$: Number of pixels in the intersection of \\(P\\) and \\(T\\).\n",
    "- $|P \\cup T|$: Number of pixels in the union of \\(P\\) and \\(T\\).\n",
    "\n",
    "**Example Use Case in Binary Segmentation**\n",
    "For binary segmentation tasks, the predicted mask is usually a binary image obtained by applying a threshold to the model's output. The ground truth mask is also a binary image. The IoU is then calculated as:\n",
    "\n",
    "1. Compute the intersection: Sum of all pixels where both the predicted and ground truth masks have value 1.\n",
    "2. Compute the union: Sum of all pixels where either the predicted or ground truth mask has value 1.\n",
    "3. Divide the intersection by the union.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question-7:** write a function that evaluates the multi-class iou score of the prediction\n",
    "```python\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "    with torch.no_grad():\n",
    "        # 1. use softmax as the activation function to convert pred logits to probs\n",
    "\n",
    "        # 2. calculate the iou score for each class and average\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):\n",
    "            # 3. caculate the ious score of current class\n",
    "            intersect =\n",
    "            union =\n",
    "\n",
    "            iou = /\n",
    "            iou_per_class.append(iou)\n",
    "\n",
    "        return np.nanmean(iou_per_class)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:15:13.931704Z",
     "start_time": "2024-12-12T21:15:13.926232Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:18:16.555148Z",
     "start_time": "2024-12-12T21:17:42.178431Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "val_ious = []\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            iou = mIoU(outputs, torch.argmax(masks,dim=1))\n",
    "            iou_scores.append(iou)\n",
    "\n",
    "    average_iou = np.mean(iou_scores)\n",
    "    val_ious.append(average_iou)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_losses[-1]:.4f}, IoU: {average_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Step-6 Improve the performance\n",
    "So here comes the question, what shall we do to improve the performance?\n",
    "1. Use more powerful architectures (UNet++)\n",
    "2. Use different loss functions (you may try dice coefficient loss)\n",
    "3. Use more data (download radiopedia data for further experiments https://www.kaggle.com/competitions/covid-segmentation/data?select=images_radiopedia.npy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
