{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaSSlLw33i6_",
        "outputId": "fec49e85-f4a1-4672-e4a2-42c2eb53f90d"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "! pip install torch\n",
        "! pip install numpy\n",
        "! pip install matplotlib\n",
        "! pip install torchvision\n",
        "! pip install voxelmorph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kJbEuNkBq_YL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ['NEURITE_BACKEND'] = 'pytorch'\n",
        "os.environ['VXM_BACKEND'] = 'pytorch'\n",
        "import voxelmorph as vxm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02L8Gq2GEIpX"
      },
      "source": [
        "## Registration Optimization\n",
        "\n",
        "Registration optimization consists of three key components:\n",
        "\n",
        "### 1. Transformation\n",
        "- **Rigid**: Translation and rotation only.\n",
        "- **Affine**: Includes scaling and shearing.\n",
        "- **Deformable**: Allows complex, non-linear transformations.\n",
        "\n",
        "### 2. Optimization\n",
        "- **Gradient-based**: E.g., Gradient Descent, L-BFGS.\n",
        "\n",
        "### 3. Cost Function\n",
        "- **Mean Squared Error (MSE)**: Suitable for intensity-based similarity.\n",
        "- **Normalized Cross Correlation (NCC)**: Suitable for Brain images.\n",
        "- **Mutual Information (MI)**: Suitable for multi-modality images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIeAugGrLasj"
      },
      "source": [
        "First generate a digital phatom of SAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "u1T5hEmHDzL1",
        "outputId": "2c6cb717-cfb7-4360-98ea-1a4e653622ae"
      },
      "outputs": [],
      "source": [
        "# Define image dimensions and center\n",
        "image_size = (40, 64)\n",
        "center = [20, 32]\n",
        "\n",
        "# LV and RV radii (adjust as needed)\n",
        "r_lv = [6, 12]  # LV endocardium and epicardium radii\n",
        "r_rv = [16, 20, 9, 12]  # RV radii for ellipse (endo_a, epi_a, endo_b, epi_b)\n",
        "\n",
        "# Create coordinate grid\n",
        "y, x = np.ogrid[-center[0]:image_size[0] - center[0], -center[1]:image_size[1] - center[1]]\n",
        "\n",
        "# Create LV masks\n",
        "mask_lv_endo = x**2 + y**2 <= r_lv[0]**2  # Endocardium\n",
        "mask_lv_epi = x**2 + y**2 <= r_lv[1]**2  # Epicardium\n",
        "\n",
        "lv_mask = np.zeros(image_size)\n",
        "lv_mask[mask_lv_epi] = 1  # Epicardium\n",
        "lv_mask[mask_lv_endo] = 0  # Hollow out the endocardium\n",
        "\n",
        "# Create RV masks (elliptical shape)\n",
        "mask_rv_epi = ((x**2) / r_rv[1]**2 + (y**2) / r_rv[3]**2) <= 1  # Epicardium\n",
        "mask_rv_endo = ((x**2) / r_rv[0]**2 + (y**2) / r_rv[2]**2) <= 1  # Endocardium\n",
        "\n",
        "rv_mask = np.zeros(image_size)\n",
        "rv_mask[mask_rv_epi] = 1  # Epicardium\n",
        "rv_mask[mask_rv_endo] = 0  # Hollow out the endocardium\n",
        "rv_mask[:, center[1]:] = 0  # Keep RV on the left half\n",
        "\n",
        "# Combine LV and RV masks\n",
        "heart_mask = lv_mask + rv_mask\n",
        "heart_mask[heart_mask > 1] = 1  # Ensure binary mask\n",
        "\n",
        "# Display the result\n",
        "plt.imshow(heart_mask, cmap='gray')\n",
        "plt.title(\"I am a binary heart image in short axis view.\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qu6twaPNvkS"
      },
      "source": [
        "# Transformation\n",
        "\n",
        "To do the subpixel translation, we need to shift the pixels, then do the intepolation.\n",
        "\n",
        "Here is the implementation of neighbour interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "84zinJKbNrFg",
        "outputId": "052605f1-5314-442e-a845-200f1a59af1b"
      },
      "outputs": [],
      "source": [
        "translated_values = np.zeros(heart_mask.shape)\n",
        "\n",
        "shift_x = 2.4\n",
        "shift_y = -7.6\n",
        "padding_value = 0\n",
        "\n",
        "\n",
        "for new_index_x in range(translated_values.shape[0]):\n",
        "    for new_index_y in range(translated_values.shape[1]):\n",
        "        previous_index_x = int(np.round(new_index_x + shift_x))\n",
        "        previous_index_y = int(np.round(new_index_y + shift_y))\n",
        "        if previous_index_x >= 0 and previous_index_x < translated_values.shape[0] and \\\n",
        "            previous_index_y >= 0 and previous_index_y < translated_values.shape[1]:\n",
        "            translated_values[new_index_x, new_index_y] = heart_mask[previous_index_x, previous_index_y]\n",
        "        else:\n",
        "            translated_values[new_index_x, new_index_y] = padding_value\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(heart_mask,\n",
        "           cmap=plt.get_cmap('gray'),\n",
        "           aspect='equal',\n",
        "           origin='lower',\n",
        "           interpolation='nearest')\n",
        "plt.title('Original Heart')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(translated_values,\n",
        "           cmap=plt.get_cmap('gray'),\n",
        "           aspect='equal',\n",
        "           origin='lower',\n",
        "           interpolation='nearest')\n",
        "plt.title('Translated Heart')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgAmPqfZTYPj"
      },
      "source": [
        "Similarly, we can do the linear interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "8sRhzzMETXTL",
        "outputId": "93b18b4f-f1d8-4585-a77c-c97b3cba5fa5"
      },
      "outputs": [],
      "source": [
        "for new_index_x in range(translated_values.shape[0]):\n",
        "    for new_index_y in range(translated_values.shape[1]):\n",
        "        previous_index_x = int(np.floor(new_index_x + shift_x))\n",
        "        previous_index_y = int(np.floor(new_index_y + shift_y))\n",
        "        relative_x = new_index_x + shift_x - previous_index_x\n",
        "        relative_y = new_index_y + shift_y - previous_index_y\n",
        "        for [a,b] in [[0,0],[0,1],[1,0],[1,1]]:\n",
        "            if previous_index_x + a >=0 and previous_index_x + a < heart_mask.shape[0] and \\\n",
        "                previous_index_y + b >= 0 and previous_index_y +b < heart_mask.shape[1]:\n",
        "                    translated_values[new_index_x,new_index_y] += \\\n",
        "                    heart_mask[previous_index_x + a, previous_index_y + b]*\\\n",
        "                        abs(1-a-relative_x)*abs(1-b-relative_y)\n",
        "                # translated_value, initialised to be zero\n",
        "                # (1- relative) * value from the floating image\n",
        "                # a = 0 add the weighted value from left (same as b,since initial is 0)\n",
        "                # a = 1 add the weighted value from right\n",
        "            else:\n",
        "                translated_values[new_index_x, new_index_y] = padding_value *\\\n",
        "                    abs(1-a-relative_x)*abs(1-b-relative_y)\n",
        "\n",
        "# show the image\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(heart_mask,\n",
        "           cmap=plt.get_cmap('gray'),\n",
        "           aspect='equal',\n",
        "           origin='lower',\n",
        "           interpolation='nearest')\n",
        "plt.title('I am the original heart.')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(translated_values,\n",
        "           cmap=plt.get_cmap('gray'),\n",
        "           aspect='equal',\n",
        "           origin='lower',\n",
        "           interpolation='nearest')\n",
        "plt.title('I am a translated heart.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-feJk2IPFrt"
      },
      "source": [
        "### 2D Affine Transformation\n",
        "\n",
        "A 2D affine transformation can be represented using a $3 \\times 3$ matrix to describe transformations like translation, rotation, scaling, shearing, and combinations thereof.\n",
        "\n",
        "The general affine transformation matrix $ \\mathbf{A} $ is:\n",
        "\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & t_x \\\\\n",
        "a_{21} & a_{22} & t_y \\\\\n",
        "0      & 0      & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Parameters:\n",
        "1. **Translation** ($t_x, t_y$): Moves the image along the x-axis and y-axis.\n",
        "2. **Rotation and Scaling** ($a_{11}, a_{22}$):\n",
        "   - These diagonal elements contribute to scaling or uniform scaling with rotation.\n",
        "3. **Shearing** ($a_{12}, a_{21}$):\n",
        "   - These off-diagonal elements introduce shearing along the x or y axes.\n",
        "\n",
        "#### Expanded Equation:\n",
        "For a point $(x, y)$ in the original image, the transformed coordinates $(x', y')$ are computed as:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x' \\\\\n",
        "y' \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & t_x \\\\\n",
        "a_{21} & a_{22} & t_y \\\\\n",
        "0      & 0      & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x \\\\\n",
        "y \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This expands to:\n",
        "\n",
        "$$\n",
        "x' = a_{11}x + a_{12}y + t_x\n",
        "$$\n",
        "$$\n",
        "y' = a_{21}x + a_{22}y + t_y\n",
        "$$\n",
        "\n",
        "### Special Cases:\n",
        "- **Pure Translation**:\n",
        "  $$\n",
        "  \\mathbf{A} =\n",
        "  \\begin{bmatrix}\n",
        "  1 & 0 & t_x \\\\\n",
        "  0 & 1 & t_y \\\\\n",
        "  0 & 0 & 1\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- **Pure Rotation** (by angle $ \\theta $):\n",
        "  $$\n",
        "  \\mathbf{A} =\n",
        "  \\begin{bmatrix}\n",
        "  \\cos\\theta & -\\sin\\theta & 0 \\\\\n",
        "  \\sin\\theta &  \\cos\\theta & 0 \\\\\n",
        "  0          &  0          & 1\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- **Scaling**:\n",
        "  $$\n",
        "  \\mathbf{A} =\n",
        "  \\begin{bmatrix}\n",
        "  s_x & 0   & 0 \\\\\n",
        "  0   & s_y & 0 \\\\\n",
        "  0   & 0   & 1\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- **Shearing**:\n",
        "  $$\n",
        "  \\mathbf{A} =\n",
        "  \\begin{bmatrix}\n",
        "  1 & k_x & 0 \\\\\n",
        "  k_y & 1 & 0 \\\\\n",
        "  0   & 0 & 1\n",
        "  \\end{bmatrix}\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr2SKM8qSoFh"
      },
      "source": [
        "## Exercise 1: Applying a 30-Degree Clock-wise Rotation to the Heart Image\n",
        "\n",
        "In this exercise, you will revise the affine matrix to apply a **30-degree rotation** to the center of the original heart image. The transformation will use **linear interpolation** to ensure smooth results.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Modify the affine transformation matrix to include a rotation by **30 degrees**.\n",
        "2. Ensure the rotation is centered on the image to maintain alignment.\n",
        "3. Use **linear interpolation** for warping the image.\n",
        "\n",
        "### Key Points\n",
        "- Rotation matrix for a 30-degree rotation is defined as:\n",
        "  $$\n",
        "  R =\n",
        "  \\begin{bmatrix}\n",
        "  \\cos\\theta & -\\sin\\theta & 0 \\\\\n",
        "  \\sin\\theta & \\cos\\theta & 0\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "  Where $\\theta = 30^\\circ$.\n",
        "- Ensure the affine matrix includes translation to keep the rotation centered.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_FeJavbUEhT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-G6mWIsU83f"
      },
      "source": [
        "### Interpolation and Transformation Customization\n",
        "\n",
        "You can modify **shearing** and **translation** parameters using different interpolation methods such as:\n",
        "\n",
        "- **Bilinear**\n",
        "- **Cubic**\n",
        "\n",
        "### Useful Packages for Interpolation and Transformation\n",
        "\n",
        "The process of interpolation and transformation has been streamlined into convenient packages. Here are some you can explore:\n",
        "\n",
        "- **`scipy.ndimage`**: Use `affine_transform` for affine transformations.\n",
        "- **`torch.nn.functional`**: Combine `affine_grid` and `grid_sample` for efficient grid-based sampling.\n",
        "\n",
        "### Important Notes\n",
        "- Be **cautious about indexing**, as it may vary between different packages and implementations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBUTtY_tWRKX"
      },
      "source": [
        "## Optimization and Loss Function\n",
        "\n",
        "Pairwise registration typically involves a pair of images: the **moving image** and the **fixed image**. The goal is to find the optimal transformation parameters that align the moving image to the fixed image by minimizing an objective loss function.\n",
        "\n",
        "Below is a custom implementation of an affine registration process:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qq1LoNtX8smy"
      },
      "outputs": [],
      "source": [
        "def affine_register(moving_img,\n",
        "                    fixed_img,\n",
        "                    lr=1E-5,\n",
        "                    epochs=1000,\n",
        "                    device='cpu',\n",
        "                    criterions=None, # one of nn.MSELoss(), NCCLoss, NMILoss()\n",
        "                    ):\n",
        "  # params initialization\n",
        "  params = torch.tensor([1.0, 0.0, 0.0,  # a, b, tx\n",
        "                        0.0, 1.0, 0.0],  # c, d, ty\n",
        "                        requires_grad=True)\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.Adam([params], lr=lr)\n",
        "\n",
        "  losses = []\n",
        "  # Optimization loop\n",
        "  for iteration in range(epochs):\n",
        "      # Create affine matrix\n",
        "      affine_matrix = params.view(2, 3).unsqueeze(0)  # Shape: (1, 2, 3)\n",
        "\n",
        "      # Warp the moving image\n",
        "      moved_grid = F.affine_grid(affine_matrix, moving_img.size(), align_corners=True)\n",
        "      moved_img = F.grid_sample(moving_img, moved_grid, align_corners=True)\n",
        "\n",
        "      # Compute similarity loss (e.g., Mean Squared Error)\n",
        "      loss = criterions(moved_img, fixed_img)\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  # plot the error.\n",
        "  plt.plot(losses, label='Error')\n",
        "  plt.title('Optimization Criterion')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  # Final transformation matrix\n",
        "  moved_affine = params.view(2, 3).detach()\n",
        "\n",
        "  # Warp the moving image with the final transformation\n",
        "  moved_grid = F.affine_grid(moved_affine.unsqueeze(0), moving_img.size(), align_corners=True)\n",
        "  moved_img = F.grid_sample(moving_img, moved_grid, align_corners=True)\n",
        "  return moved_img, moved_affine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6PH5kk88Td7d"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "fixed_img = torch.Tensor(heart_mask).unsqueeze(0).unsqueeze(0).to(device)\n",
        "moving_img = torch.Tensor(translated_values).unsqueeze(0).unsqueeze(0).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMOE0ih1BJAU"
      },
      "source": [
        "## Exercise 2: Defining and Using MSE Loss\n",
        "\n",
        "In this exercise, we define the **Mean Squared Error (MSE)** as the loss function and use the `affine_register` function to optimize the transformation parameters that align the moving image to the fixed image.\n",
        "\n",
        "The **Mean Squared Error (MSE)** measures the average squared difference between corresponding pixels of two images. The formula for MSE is given by:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\text{moved}_i - \\text{fixed}_i \\right)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\text{moved}_i$: Pixel intensity of the moving image after transformation.\n",
        "- $\\text{fixed}_i$: Pixel intensity of the fixed reference image.\n",
        "- $N$: Total number of pixels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEYBR-vqBM6s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Ekcyf6B4ht"
      },
      "source": [
        "## Exercise 3: Revising to Rigid Transformation\n",
        "\n",
        "In this exercise, we revise the `affine_register` function to create a `rigid_register` function. For **rigid transformations**, the transformation is limited to **rotation** and **translation**. This means scaling and shearing are not allowed, reducing the number of free parameters to **three**: $ \\theta, t_x, t_y $.\n",
        "\n",
        "### Definition of Rigid Transformation\n",
        "\n",
        "The rigid transformation for 2D images is represented by the following equation:\n",
        "\n",
        "$$\n",
        "T_{\\text{rigid}} =\n",
        "\\begin{bmatrix}\n",
        "\\cos\\theta & -\\sin\\theta & t_x \\\\\n",
        "\\sin\\theta & \\cos\\theta & t_y\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\theta $: The rotation angle.\n",
        "- $ t_x, t_y $: The translation in the x and y directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sL0gluFB32n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM0xylftYZBW"
      },
      "source": [
        "## Exercise 4: Defining NCC Loss and Using `rigid_register`\n",
        "\n",
        "In this exercise, we define the **Normalized Cross-Correlation (NCC)** as the loss function and use the `rigid_register` function to optimize the transformation parameters. If the `rigid_register` function fails, you can alternatively refer back to the `affine_register` function.\n",
        "\n",
        "### What is NCC?\n",
        "\n",
        "The **Normalized Cross-Correlation (NCC)** measures the similarity between two images, accounting for differences in mean and variance. NCC is a commonly used similarity metric in image registration, especially for intensity-based methods.\n",
        "\n",
        "The formula for NCC is:\n",
        "\n",
        "$$\n",
        "\\text{NCC} = \\frac{\\text{Covariance}(moved, fix)}{\\sqrt{\\text{Var}(moved) \\cdot \\text{Var}(fix)} + \\epsilon}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\text{Covariance}(moved, fix) $: Measures the relationship between the pixel intensities of the moving and fixed images.\n",
        "- $ \\text{Var}(moved) $: Variance of the moving image.\n",
        "- $ \\text{Var}(fix) $: Variance of the fixed image.\n",
        "- $ \\epsilon $: A small constant to avoid division by zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u282uSfroVv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J_JL3eGtsT7"
      },
      "source": [
        "## Exercise 5: Define Mutual Information (MI) and Using `affine_register` on real-world Cine and T2 Cardiac Magnetic Resonance (CMR) images\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, we define the **Mutual Information (MI)** as the loss function and use the `affine_register` function to optimize the transformation parameters on real cardiac magentic resonance images (cine and T2).\n",
        "\n",
        "We now transition from **binary CMR images** to **real-world Cine and T2 CMR images**, adopted from the public **MSCMR dataset**. Despite the use of motion control techniques such as **breath-holding** and **cardiac triggering**, shifts remain a common occurrence, necessitating robust image registration techniques.\n",
        "\n",
        "Unlike the simplified digital phantoms we previously created, these real-world images feature **complex backgrounds**, introducing significant challenges to the registration process. These complexities highlight the need for advanced and adaptable registration frameworks capable of handling variability and noise in real-collected datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### What is Mutual Information?\n",
        "\n",
        "**Mutual Information (MI)** measures the amount of information shared between two images. It is defined as:\n",
        "\n",
        "$$\n",
        "I(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y) + \\epsilon}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $X$ and $Y$: Intensity values of the moving and fixed images, respectively.\n",
        "- $p(x, y)$: Joint probability distribution of $X$ and $Y$.\n",
        "- $p(x)$ and $p(y)$: Marginal probability distributions of $X$ and $Y$.\n",
        "- $\\epsilon$: A small constant to avoid division by zero.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Use MI?\n",
        "\n",
        "- **Robust to Intensity Differences**: MI is ideal for registering images with different intensity distributions (e.g., multi-modal MRI or CT images).\n",
        "- **Nonlinear Relationships**: Unlike Mean Squared Error or NCC, MI can capture complex, nonlinear relationships between pixel intensities.\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation Steps\n",
        "0. Remember to upload the 'realSCMR.npz' to your Google Drive.\n",
        "1. Compute the joint histogram of the moving and fixed images.\n",
        "2. Normalize the histogram to get the joint probability distribution, $p(x, y)$.\n",
        "3. Calculate the marginal distributions, $p(x)$ and $p(y)$.\n",
        "4. Compute the MI using the formula above.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPRzBzMm3yph",
        "outputId": "d77d1d19-58e6-4071-9e9c-b8e40158f46b"
      },
      "outputs": [],
      "source": [
        "# upload npz\n",
        "MSCMR = np.load('realSCMR.npz')\n",
        "print(MSCMR.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "BpV2b14C41tT",
        "outputId": "39f9bf2d-ca40-45e5-c20f-11b776bb4d27"
      },
      "outputs": [],
      "source": [
        "# plot the three images\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(MSCMR['C0'],cmap = 'gray')\n",
        "plt.title('Cine')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(MSCMR['T2'],cmap = 'gray')\n",
        "plt.title('T2')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(MSCMR['T2_transformed'],cmap = 'gray')\n",
        "plt.title('T2 Transformed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-I7MjgPtGPh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoDLHKDic3yC"
      },
      "source": [
        "# Deep-Learning Baseline: VoxelMorph with PyTorch\n",
        "\n",
        "VoxelMorph is a deep-learning-based framework for image registration. It uses a U-Net-like architecture to predict dense deformation fields for aligning images, making it faster and more scalable compared to traditional methods. Below, we demonstrate its application using PyTorch and the MNIST dataset as a simplified example.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Features of VoxelMorph\n",
        "\n",
        "1. **End-to-End Learning**:\n",
        "   - Learns a dense deformation field directly from the data in a single forward pass.\n",
        "2. **CNN-Based**:\n",
        "   - Employs a U-Net to predict the deformation field.\n",
        "3. **Loss Function**:\n",
        "   - Combines similarity loss (e.g., Mean Squared Error) with a regularization term to enforce smooth deformations.\n",
        "4. **Applications**:\n",
        "   - Medical imaging (e.g., MRI, CT scans), computer vision tasks requiring alignment.\n",
        "\n",
        "---\n",
        "\n",
        "## VoxelMorph Workflow\n",
        "\n",
        "1. **Input**: A pair of images (e.g., a moving image and a fixed image).\n",
        "2. **Network**: A U-Net predicts a dense deformation field.\n",
        "3. **Transformation**: The deformation field is applied to the moving image to align it with the fixed image.\n",
        "4. **Loss Function**:\n",
        "   - Similarity Loss: Ensures alignment of the two images.\n",
        "   - Regularization Loss: Encourages smoothness in the deformation field.\n",
        "5. **Output**: The warped image and the deformation field.\n",
        "\n",
        "---\n",
        "\n",
        "## Implementation with PyTorch\n",
        "\n",
        "Below is a PyTorch implementation of VoxelMorph using the MNIST dataset:\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "We use pairs of MNIST digits as an illustrative example for image registration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXE9ELqUcUst"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GXO3GNmZedg",
        "outputId": "3cd59fb6-7d41-448e-cc70-211c5162cbf6"
      },
      "outputs": [],
      "source": [
        "#Downloading MNIST data\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "# take the digit with 5 out of the training dataset\n",
        "digit_5 = train_data.data[train_data.targets == 5]\n",
        "print(digit_5.shape)\n",
        "# normalize the digit_5 to [0,1]\n",
        "digit_5 = digit_5.float()/255\n",
        "# just use the first 1000 samples for experiments\n",
        "digit_5 = digit_5[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUHqCtI0fydL"
      },
      "outputs": [],
      "source": [
        "nb_val = 200  # keep smaller number of samples for a faster training.\n",
        "nb_tst = 100\n",
        "\n",
        "x_trn = digit_5[:-(nb_val + nb_tst), ...]\n",
        "x_val = digit_5[-(nb_val + nb_tst): -nb_tst, ...]\n",
        "x_tst = digit_5[-nb_tst:, ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqsUKlYVgl2V",
        "outputId": "b695643b-40da-4aa2-8e64-85bef4306236"
      },
      "outputs": [],
      "source": [
        "pad_amount = ((0, 0), (2,2), (2,2))\n",
        "\n",
        "# fix data\n",
        "x_trn = np.pad(x_trn, pad_amount, 'constant')\n",
        "x_val = np.pad(x_val, pad_amount, 'constant')\n",
        "x_tst = np.pad(x_tst, pad_amount, 'constant')\n",
        "\n",
        "# verify\n",
        "print('shape of training data', x_trn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZe_iRY5ILQb",
        "outputId": "eb92db58-a294-4818-d8b8-bd2d321eee37"
      },
      "outputs": [],
      "source": [
        "# configure unet input shape\n",
        "ndim = 2\n",
        "inshape = x_trn.shape[1:]\n",
        "\n",
        "# configure unet features\n",
        "# unet architecture\n",
        "enc_nf = [32, 32, 32]\n",
        "dec_nf = [32, 32, 32, 32, 16]\n",
        "\n",
        "model = vxm.networks.VxmDense(\n",
        "    inshape=inshape,\n",
        "    nb_unet_features=[enc_nf, dec_nf],\n",
        "    bidir=False,\n",
        "    int_steps=0,\n",
        "    int_downsize=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAW_Cce7IrOb"
      },
      "outputs": [],
      "source": [
        "def vxm_data_generator(x_data, batch_size=1):\n",
        "    \"\"\"\n",
        "    Generator that takes in data of size [N, H, W], and yields data for\n",
        "    our custom vxm model. Note that we need to provide numpy data for each\n",
        "    input, and each output.\n",
        "\n",
        "    inputs:  moving [bs, H, W, 1], fixed image [bs, H, W, 1]\n",
        "    outputs: moved image [bs, H, W, 1], zero-gradient [bs, H, W, 2]\n",
        "    \"\"\"\n",
        "\n",
        "    # preliminary sizing\n",
        "    vol_shape = x_data.shape[1:] # extract data shape\n",
        "    ndims = len(vol_shape)\n",
        "\n",
        "    # prepare a zero array the size of the deformation\n",
        "    # we'll explain this below\n",
        "    zero_phi = np.zeros([batch_size, *vol_shape, ndims])\n",
        "\n",
        "    while True:\n",
        "        # prepare inputs:\n",
        "        # images need to be of the size [batch_size, H, W, 1]\n",
        "        idx1 = np.random.randint(0, x_data.shape[0], size=batch_size)\n",
        "        moving_images = x_data[idx1, ..., np.newaxis]\n",
        "        idx2 = np.random.randint(0, x_data.shape[0], size=batch_size)\n",
        "        fixed_images = x_data[idx2, ..., np.newaxis]\n",
        "        inputs = [moving_images, fixed_images]\n",
        "\n",
        "        # prepare outputs (the 'true' moved image):\n",
        "        # of course, we don't have this, but we know we want to compare\n",
        "        # the resulting moved image with the fixed image.\n",
        "        # we also wish to penalize the deformation field.\n",
        "        outputs = [fixed_images, zero_phi]\n",
        "\n",
        "        yield (inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GBcJQ8xJVdH"
      },
      "outputs": [],
      "source": [
        "# let's train\n",
        "train_generator = vxm_data_generator(x_trn)\n",
        "in_sample, out_sample = next(train_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhX36QNdZA3"
      },
      "source": [
        "Define the loss, configs and start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26aCMzK1JxZd"
      },
      "outputs": [],
      "source": [
        "losses = [nn.MSELoss()]\n",
        "weights = [1]\n",
        "epochs = 200\n",
        "learning_rate = 1E-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEkYuodFJgAr"
      },
      "outputs": [],
      "source": [
        "# prepare the model for training and send to device\n",
        "model.to(device)\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_loss = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_loss = []\n",
        "    epoch_total_loss = []\n",
        "    epoch_step_time = []\n",
        "\n",
        "    for step in range(5):\n",
        "\n",
        "        # generate inputs (and true outputs) and convert them to tensors\n",
        "        inputs, y_true = next(train_generator)\n",
        "        inputs = [torch.from_numpy(d).to(device).float().permute(0, 3, 1, 2) for d in inputs]\n",
        "        y_true = [torch.from_numpy(d).to(device).float().permute(0, 3, 1, 2) for d in y_true]\n",
        "\n",
        "        # run inputs through the model to produce a warped image and flow field\n",
        "        y_pred = model(*inputs)\n",
        "\n",
        "        # calculate total loss\n",
        "        loss = 0\n",
        "        loss_list = []\n",
        "\n",
        "        for n, loss_function in enumerate(losses):\n",
        "            curr_loss = loss_function(y_true[n], y_pred[n]) * weights[n]\n",
        "            loss_list.append(curr_loss.item())\n",
        "            loss += curr_loss\n",
        "\n",
        "        epoch_loss.append(loss_list)\n",
        "        epoch_total_loss.append(loss.item())\n",
        "\n",
        "        # backpropagate and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    total_loss.append(np.mean(epoch_total_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gzX2iAD8LVcb",
        "outputId": "ebc9965c-76c0-4615-f048-e6b76833a6df"
      },
      "outputs": [],
      "source": [
        "# plot error\n",
        "plt.plot(total_loss, label='Error')\n",
        "plt.title('Optimization Criterion')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "sHtkvhKoeChD",
        "outputId": "1891b850-020e-463c-da0d-326ee907e1c0"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "\n",
        "val_generator = vxm_data_generator(x_val)\n",
        "in_sample, out_sample = next(val_generator)\n",
        "\n",
        "# Convert inputs and true outputs to tensors\n",
        "inputs = [torch.from_numpy(d).to(device).float().permute(0, 3, 1, 2) for d in in_sample]\n",
        "y_true = [torch.from_numpy(d).to(device).float().permute(0, 3, 1, 2) for d in out_sample]\n",
        "\n",
        "# Forward pass through the model to generate predictions\n",
        "with torch.no_grad():  # Disable gradients for evaluation\n",
        "    y_pred = model(*inputs)\n",
        "\n",
        "# plot the images\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(in_sample[0][0], cmap=plt.get_cmap('gray'), aspect='equal',\n",
        "           origin='lower', interpolation='nearest')\n",
        "plt.title('moving')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(y_pred[0].squeeze().cpu().detach().numpy(), cmap=plt.get_cmap('gray'), aspect='equal',\n",
        "           origin='lower', interpolation='nearest')\n",
        "plt.title('moved')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(y_true[0].squeeze().cpu().detach().numpy(), cmap=plt.get_cmap('gray'), aspect='equal',\n",
        "           origin='lower', interpolation='nearest')\n",
        "plt.title('fixed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPAeet0XPCSN"
      },
      "source": [
        "## Exercise 7 (Challenging): Train and Validate VoxelMorph with the MedMNIST Dataset\n",
        "\n",
        "In this exercise, we will tackle the challenge of medical imaging registration using the **MedMNIST** dataset. MedMNIST is a large-scale, lightweight benchmark designed for biomedical image classification and segmentation tasks. Specifically, we will train and validate a **VoxelMorph** model to align medical images, leveraging the diverse subsets and labels provided by MedMNIST. This hands-on exercise combines state-of-the-art registration techniques with the versatility of the MedMNIST dataset to deepen your understanding of medical image analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR_cgjPcnrpo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
