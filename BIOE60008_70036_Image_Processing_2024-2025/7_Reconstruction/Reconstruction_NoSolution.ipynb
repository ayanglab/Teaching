{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Before We Start\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "418abc04cfafddb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install Required Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c926c5e9ad9593d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-08T11:16:08.413549Z",
     "start_time": "2025-01-08T11:16:08.389118Z"
    }
   },
   "id": "286ac46ba7d017f8",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "Let's start by importing the required libraries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e53e0ca416faa9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "from glob import glob\n",
    "from scipy.fftpack import fftshift, ifftshift, fftn, ifftn\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.nn import functional as F\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2085da592c2e35c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Working Path (for Google Colab)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "766066c3c13ecb90"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# Path to the directory containing the data\n",
    "# path = './gdrive/MyDrive/Share/MRI_RECON_TUTORIAL/simple_mri_recon/' \n",
    "# os.chdir(path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed2858d8ab7503e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup Random Seed\n",
    "\n",
    "We'll set a random seed for reproducibility."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f72f05672740ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7aefd25ca7703af",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup Device\n",
    "\n",
    "We'll check if a GPU is available and set the device accordingly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9f4ff9687b9793d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a3d9da6d121849",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some Utility Functions\n",
    "\n",
    "Let's define some utility functions that we'll use throughout the tutorial."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd81f6c91973b7c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create directory\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8da8fcf94355f91c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and Visualize a Sample\n",
    "\n",
    "MRI data is loaded in .h5 format, containing complex-valued images. Magnitude and phase components are extracted for visualization. The magnitude and phase images reveal important structural and phase information, offering insights into the data quality and characteristics.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb2a4c88ae7a5dab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load H5 file\n",
    "def read_processed_h5(data_path):\n",
    "    with h5py.File(data_path, 'r') as file:\n",
    "        data = {\n",
    "            'image_complex': file['image_complex'][()],\n",
    "            'data_name': file['image_complex'].attrs['data_name'],\n",
    "            'slice_idx': file['image_complex'].attrs['slice_idx'],\n",
    "        }\n",
    "    return data\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a2e5e7b4d1dc486",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1: Design a Normalisation Function\n",
    "\n",
    "You need to normalise the complex-valued MRI data before visualisation. Implement a normalisation function that takes the complex-valued image and normalises it to a range of [0, 1]. The function should support normalisation based on the magnitude component.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34117c7c2901aaf6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "def preprocess_normalisation(img, type):\n",
    "    if type == 'complex_mag':\n",
    "        # Add your code here...\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return img\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f2b48f542485e96"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load H5 data\n",
    "sample_data = read_processed_h5('sample/file1000031_000.h5')\n",
    "\n",
    "# Load complex MR data\n",
    "image_complex = sample_data['image_complex']\n",
    "\n",
    "# Do normalisation\n",
    "image_complex_norm = preprocess_normalisation(image_complex, type='complex_mag')\n",
    "\n",
    "# Show some information before and after normalisation\n",
    "print(\"Data Shape: {}; Data Type: {}; \"\n",
    "      \"Data Range (before normalisation): {:.3e}~{:.3e}; \"\n",
    "      \"Data Range (after normalisation): {:.3e}~{:.3e}\".format(image_complex.shape, \n",
    "                                                               image_complex.dtype, \n",
    "                                                               abs(image_complex).min(), \n",
    "                                                               abs(image_complex).max(), \n",
    "                                                               abs(image_complex_norm).min(), \n",
    "                                                               abs(image_complex_norm).max(),))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e6d4d69e230f7aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract components: magnitude and phase\n",
    "image_magnitude = np.abs(image_complex_norm)\n",
    "image_phase = np.angle(image_complex_norm)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Magnitude image\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(image_magnitude, cmap='gray')\n",
    "plt.title('Magnitude Image')\n",
    "plt.axis('off')\n",
    "# Phase image\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(image_phase, cmap='gray')\n",
    "plt.title('Phase Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb1eba2b08e7828e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Space Fundamentals\n",
    "\n",
    "### Fourier Transform\n",
    "\n",
    "K-space is the frequency domain representation of MRI data. Visualizing k-space in both 2D and 3D provides insights into frequency content. Forward and inverse Fourier transforms allow switching between image and k-space domains, enabling operations like undersampling.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9762cd69b69e6136"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_kspace_and_mesh(kspace_magnitude):\n",
    "    kspace_magnitude_clip_for_vis = np.clip(kspace_magnitude, 0, np.percentile(kspace_magnitude, 99.9))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Plot 2D k-space magnitude\n",
    "    axes[0].imshow(kspace_magnitude_clip_for_vis, cmap='gray')\n",
    "    axes[0].set_title('K-Space Magnitude')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot 3D Mesh of the k-space magnitude\n",
    "    x = np.arange(kspace_magnitude_clip_for_vis.shape[1])\n",
    "    y = np.arange(kspace_magnitude_clip_for_vis.shape[0])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.plot_surface(x, y, kspace_magnitude_clip_for_vis, cmap='viridis')\n",
    "    ax.set_title('3D Mesh of K-Space Magnitude')\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd1f279fdc9fafc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Perform FFT to acquire k-space\n",
    "kspace_complex = fftshift(fftn(ifftshift(image_complex_norm), axes=(-2, -1)))\n",
    "\n",
    "# Plot the k-space \n",
    "kspace_magnitude = np.abs(kspace_complex)\n",
    "plot_kspace_and_mesh(kspace_magnitude)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31239114b98e897",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-space Undersampling Pattern\n",
    "\n",
    "Undersampling masks, such as GRAPPA-like or random Gaussian, are loaded. These masks define the specific sampling patterns applied to k-space data, simulating accelerated acquisition.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af3de643c8002c98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Undersampling mask selection\n",
    "def define_mask(mask_name):\n",
    "\n",
    "    # GRAPPA-like (with ACS) Regular Acceleration Factor x Central Fraction x PE (from fastMRI)\n",
    "    if mask_name == 'fMRI_Reg_AF2_CF0.16_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'regular', 'regular_af2_cf0.16_pe320.npy'))\n",
    "    elif mask_name == 'fMRI_Reg_AF4_CF0.08_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'regular', 'regular_af4_cf0.08_pe320.npy'))\n",
    "    elif mask_name == 'fMRI_Reg_AF8_CF0.04_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'regular', 'regular_af8_cf0.04_pe320.npy'))\n",
    "    elif mask_name == 'fMRI_Reg_AF16_CF0.02_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'regular', 'regular_af16_cf0.02_pe320.npy'))\n",
    "\n",
    "    # GRAPPA-like (with ACS) Random (Gaussian) Acceleration Factor x Central Fraction x PE (from fastMRI)\n",
    "    elif mask_name == 'fMRI_Ran_AF2_CF0.16_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'random', 'random_af2_cf0.16_pe320.npy'))\n",
    "    elif mask_name == 'fMRI_Ran_AF4_CF0.08_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'random', 'random_af4_cf0.08_pe320.npy'))\n",
    "    elif mask_name == 'fMRI_Ran_AF8_CF0.04_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'random', 'random_af8_cf0.04_pe320.npy'))\n",
    "    elif mask_name == 'fMRI_Ran_AF16_CF0.02_PE320':\n",
    "        mask_1d = np.load(os.path.join('mask', 'fastmri', 'random', 'random_af16_cf0.02_pe320.npy'))\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    mask_1d = mask_1d[:, np.newaxis]\n",
    "    mask = np.repeat(mask_1d, 320, axis=1).transpose((1, 0))\n",
    "    \n",
    "    return mask\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d58839a9e217d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load k-space undersampling pattern\n",
    "mask = define_mask(mask_name='fMRI_Reg_AF4_CF0.08_PE320')\n",
    "\n",
    "# Plot the mask\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('K-Space Undersampling Mask')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01e2a86babd2441",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-space Undersampling\n",
    "\n",
    "The undersampling mask is applied to the fully sampled k-space data. This step simulates accelerated MRI acquisition, introducing aliasing artifacts. An inverse FFT reconstructs the image from undersampled k-space data, revealing the extent of artifacts caused by undersampling.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b4bac2419d837c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# k-space undersampling\n",
    "undersampled_kspace_complex = kspace_complex * mask\n",
    "\n",
    "# Plot the undersampled k-space \n",
    "undersampled_kspace_magnitude = np.abs(undersampled_kspace_complex)\n",
    "plot_kspace_and_mesh(undersampled_kspace_magnitude)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "879fa2d913074f3e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Perform Inverse FFT to acquire the undersampled image\n",
    "undersampled_image_complex = fftshift(ifftn(ifftshift(undersampled_kspace_complex), axes=(-2, -1)))\n",
    "\n",
    "# Extract components: magnitude, phase images\n",
    "undersampled_image_magnitude = np.abs(undersampled_image_complex)\n",
    "undersampled_image_phase = np.angle(undersampled_image_complex)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Fullysampled image\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(image_magnitude, cmap='gray')\n",
    "plt.title('Fullysampled Image')\n",
    "plt.axis('off')\n",
    "# Undersampled image\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(undersampled_image_magnitude, cmap='gray')\n",
    "plt.title('Undersampled Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d3a9a0a11279b46",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c887313fa01abbfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training an End-to-end MRI Reconstruction Model\n",
    "\n",
    "Deep learning models, particularly U-Net variants, are employed for reconstructing undersampled MRI images. This section details the model definition, data preparation, and training steps.\n",
    "\n",
    "In a deep learning-based MRI reconstruction pipeline, the input and target are critical for training the model:\n",
    "\n",
    "- **Input**: The undersampled MRI image or k-space data. This is generated by applying an undersampling mask to the fully sampled k-space data. For training, the input is typically represented in the image domain as two channels: real and imaginary components of the complex-valued image.\n",
    "\n",
    "- **Target**: The fully sampled MRI image or k-space data, representing the ground truth. Like the input, the target is often stored in the image domain with real and imaginary components.\n",
    "\n",
    "The goal of training is for the model to learn a mapping from the undersampled input to the fully reconstructed target.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f432d0d0d4b03d66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete the previous variables\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34853b1cec8b030e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "del image_complex, image_complex_norm, image_magnitude, image_phase\n",
    "del kspace_complex, kspace_magnitude, undersampled_kspace_complex, undersampled_kspace_magnitude\n",
    "del undersampled_image_complex, undersampled_image_magnitude, undersampled_image_phase\n",
    "del sample_data\n",
    "del mask\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51350152691446c6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameters\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d788f98c65cc5af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "## Task \n",
    "model_name = 'unet'\n",
    "# model_name = 'unet_resi'\n",
    "mask_name = 'fMRI_Reg_AF4_CF0.08_PE320'\n",
    "\n",
    "is_debug = False\n",
    "is_save_models = True\n",
    "is_save_all_models = False\n",
    "is_skip_training = False\n",
    "pretrained_model_path = None\n",
    "# pretrained_model_path = 'weight/unet_fMRI_Reg_AF4_CF0.08_PE320/unet.pth'\n",
    "\n",
    "task_name = '{}_{}'.format(model_name, mask_name) if not is_debug else 'tmp'\n",
    "save_path = os.path.join('runs', task_name)\n",
    "\n",
    "## Dataset\n",
    "train_path = 'dataset/fastmri_tiny/train'\n",
    "test_path = 'dataset/fastmri_tiny/val'\n",
    "train_batch_size = 4\n",
    "test_batch_size = 1\n",
    "\n",
    "## Model\n",
    "in_channels = 2\n",
    "out_channels = 2\n",
    "\n",
    "## Optimisation\n",
    "total_epochs = 100\n",
    "lr_G = 1e-4\n",
    "lr_D = 1e-4\n",
    "\n",
    "## Parameter check\n",
    "assert is_skip_training == (pretrained_model_path is not None)  # is_skip_training implies pretrained_model_path is not None\n",
    "assert not is_save_all_models or is_save_models  # is_save_all_models implies is_save_models\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c1fdc23a695561f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Network\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94a82489402fb90d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def conv_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.middle = conv_block(512, 1024)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(1024, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "\n",
    "        m = self.middle(self.pool(e4))\n",
    "\n",
    "        d4 = self.up4(m)\n",
    "        d4 = torch.cat((e4, d4), dim=1)\n",
    "        d4 = self.decoder4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat((e3, d3), dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat((e2, d2), dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat((e1, d1), dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "\n",
    "        out = self.final(d1)\n",
    "        \n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1114b504406d0d6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 2: Make U-Net Model to a Refinement U-Net Model Using Residual Learning\n",
    "\n",
    "Implement a refinement U-Net model by adding residual connections to the U-Net model. The residual connections should be added between the input and output.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "644579359839cb8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# U-Net model with skip connections\n",
    "class UNet_resi(nn.Module):\n",
    "    # Add your code here...\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc5e3d7502960747"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Network selection\n",
    "def define_model(model_name):\n",
    "\n",
    "    if model_name == 'unet':\n",
    "        model_type = UNet\n",
    "    elif model_name == 'unet_resi':\n",
    "        model_type = UNet_resi\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model_type\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1366750877f66a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_type = define_model(model_name)\n",
    "\n",
    "# Instantiate the model\n",
    "model = model_type(in_channels=in_channels, out_channels=out_channels).to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ebc91f640fa6953",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Dataloader\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d38085397403876"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# k-space undersampling\n",
    "def undersample_kspace(x, mask):\n",
    "\n",
    "    fft = fftshift(fftn(ifftshift(x, axes=(-2, -1)), axes=(-2, -1)), axes=(-2, -1))\n",
    "    fft = fft * mask\n",
    "    x = fftshift(ifftn(ifftshift(fft, axes=(-2, -1)), axes=(-2, -1)), axes=(-2, -1))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# Data loader\n",
    "class DatasetFastMRI(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_src, mask_name, is_debug=False):\n",
    "        super(DatasetFastMRI, self).__init__()\n",
    "\n",
    "        # get undersampling mask\n",
    "        self.mask = define_mask(mask_name)\n",
    "\n",
    "        # get data path\n",
    "        self.data_paths = glob(os.path.join(data_path_src, '*.h5'))\n",
    "        \n",
    "        # for debug\n",
    "        self.data_paths = self.data_paths[:12] if is_debug else self.data_paths\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # get undersampling mask\n",
    "        mask = self.mask  # H, W\n",
    "\n",
    "        # get gt image\n",
    "        H_path = self.data_paths[index]\n",
    "\n",
    "        # read h5\n",
    "        img_dict = read_processed_h5(H_path)\n",
    "\n",
    "        # get complex image\n",
    "        img_H_SC = img_dict['image_complex']\n",
    "\n",
    "        # normalisation\n",
    "        img_H_SC = preprocess_normalisation(img_H_SC, type='complex_mag')\n",
    "\n",
    "        # get zf image\n",
    "        img_L_SC = undersample_kspace(img_H_SC, mask)\n",
    "\n",
    "        # expand dim\n",
    "        img_H_SC = img_H_SC[:, :, np.newaxis]  # H, W, 1\n",
    "        img_L_SC = img_L_SC[:, :, np.newaxis]  # H, W, 1\n",
    "\n",
    "        # Complex --> 2CH\n",
    "        img_H_SC = np.concatenate((np.real(img_H_SC), np.imag(img_H_SC)), axis=-1)  # H, W, 2\n",
    "        img_L_SC = np.concatenate((np.real(img_L_SC), np.imag(img_L_SC)), axis=-1)  # H, W, 2\n",
    "\n",
    "        # HWC to CHW, numpy to tensor\n",
    "        img_L_SC = torch.from_numpy(np.ascontiguousarray(img_L_SC)).permute(2, 0, 1).to(torch.float32)\n",
    "        img_H_SC = torch.from_numpy(np.ascontiguousarray(img_H_SC)).permute(2, 0, 1).to(torch.float32)\n",
    "\n",
    "        return img_L_SC, img_H_SC   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d3999022a90d64f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training and testing dataloaders\n",
    "train_dataset = DatasetFastMRI(data_path_src=train_path, mask_name=mask_name, is_debug=is_debug)\n",
    "test_dataset = DatasetFastMRI(data_path_src=test_path, mask_name=mask_name, is_debug=is_debug)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, drop_last=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79266da8cc2f9a2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Loss Function\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d7296d7b4119b65"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Loss selection\n",
    "def define_loss(loss_type):\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        lossfn = nn.L1Loss()\n",
    "    elif loss_type == 'l2':\n",
    "        lossfn = nn.MSELoss()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return lossfn\n",
    "\n",
    "\n",
    "# FFT for frequency loss\n",
    "def fft_map(x):\n",
    "    fft_x = torch.fft.fftn(x)\n",
    "    fft_x_real = fft_x.real\n",
    "    fft_x_imag = fft_x.imag\n",
    "\n",
    "    return fft_x_real, fft_x_imag\n",
    "\n",
    "\n",
    "# Total loss\n",
    "def total_loss(predict, \n",
    "               target, \n",
    "               loss_image_weight=15,\n",
    "               loss_image_type='l1',\n",
    "               loss_freq_weight=0.1, \n",
    "               loss_freq_type='l1',\n",
    "               device='cpu'):\n",
    "\n",
    "\n",
    "    lossfn_image = define_loss(loss_image_type).to(device)\n",
    "    loss_image = lossfn_image(predict, target)\n",
    "    \n",
    "    lossfn_freq = define_loss(loss_freq_type).to(device)\n",
    "    target_k_real, target_k_imag = fft_map(target)\n",
    "    predict_k_real, predict_k_imag = fft_map(predict)\n",
    "    loss_freq = (lossfn_freq(predict_k_real, target_k_real) + lossfn_freq(predict_k_imag, target_k_imag)) / 2\n",
    "\n",
    "    return loss_image_weight * loss_image + loss_freq_weight * loss_freq \n",
    "\n",
    "# Define loss function\n",
    "loss_fn = partial(total_loss, \n",
    "                  loss_image_weight=15,\n",
    "                  loss_image_type='l1',\n",
    "                  loss_freq_weight=0.1, \n",
    "                  loss_freq_type='l1',\n",
    "                  device=device)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af77d2acb103c1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Optimisation Setting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8a453d14c6821"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b869c89fdb073ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Loop\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7f60a54754bec67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, epoch, total_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_batch_losses = []\n",
    "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{total_epochs}\", leave=False):\n",
    "        input, gt = batch\n",
    "        input, gt = input.to(device), gt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input)\n",
    "        loss = loss_fn(pred, gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        all_batch_losses.append(loss.item())\n",
    "    return total_loss / len(dataloader), all_batch_losses\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b989dbc66562ff4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Skip training\n",
    "if is_skip_training:\n",
    "    print(\"Skip training\")\n",
    "else:\n",
    "    print(\"Start training\")\n",
    "\n",
    "    train_losses_over_epochs = []\n",
    "    train_losses_over_steps = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(total_epochs):\n",
    "        # print(f\"Epoch {epoch+1}\")\n",
    "        train_loss, batch_losses = train_one_epoch(model, train_loader, optimizer, loss_fn, epoch, total_epochs)\n",
    "        train_losses_over_epochs.append(train_loss)\n",
    "        train_losses_over_steps.extend(batch_losses)\n",
    "        # print(f\"  Training Loss = {train_loss:.4f}\")\n",
    "        \n",
    "        # save model\n",
    "        mkdir(save_path)\n",
    "        if is_save_models:\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, f\"{model_name}.pth\"))\n",
    "            if is_save_all_models:\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, f\"{model_name}_{epoch+1}.pth\"))\n",
    "    \n",
    "    print(\"Training Completed\")\n",
    "    \n",
    "    # Plot training loss over epochs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_over_epochs) + 1), train_losses_over_epochs, marker='o')\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training loss over steps\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_over_steps) + 1), train_losses_over_steps, marker='o', markersize=2)\n",
    "    plt.title(\"Training Loss Over Steps\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8d794802c4839834",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metrics "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6332509b8e4ec534"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PSNR\n",
    "def calculate_psnr_single(img1, img2, data_range=None):\n",
    "\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "\n",
    "    return peak_signal_noise_ratio(img1, img2, data_range=data_range)\n",
    "\n",
    "# SSIM\n",
    "def calculate_ssim_single(img1, img2, data_range=None):\n",
    "    pass\n",
    "\n",
    "# LPIPS\n",
    "def calculate_lpips_single(img1, img2, data_range=None):\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "907a41fc1441be1b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a073cf7c00fb23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3: Add More Metrics for Evaluation\n",
    "\n",
    "3.1 Add SSIM metric for evaluation.\n",
    "You can use the `structural_similarity` function from the `skimage.metrics` module to calculate the SSIM metric. The function takes two images as input and returns the SSIM value.\n",
    "\n",
    "3.2 Add LPIPS metric for evaluation.\n",
    "You can use the LPIPS metric for evaluation. The LPIPS metric measures the perceptual similarity between two images. (https://github.com/richzhang/PerceptualSimilarity) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a64c782ab83e1c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Validation for testset\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['psnr'] = []\n",
    "    metrics_dict['psnr_zf'] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            input, gt = batch\n",
    "            input, gt = input.to(device), gt.to(device)\n",
    "            pred = model(input)\n",
    "\n",
    "            input = torch.abs(torch.complex(input[:, :1, ...], input[:, 1:, ...]))\n",
    "            pred = torch.abs(torch.complex(pred[:, :1, ...], pred[:, 1:, ...]))\n",
    "            gt = torch.abs(torch.complex(gt[:, :1, ...], gt[:, 1:, ...]))\n",
    "            \n",
    "            input = input.squeeze().float().cpu().numpy()\n",
    "            pred = pred.data.squeeze().float().cpu().numpy()\n",
    "            gt = gt.data.squeeze().float().cpu().numpy()\n",
    "            \n",
    "            psnr = calculate_psnr_single(pred, gt, data_range=gt.max())\n",
    "            metrics_dict['psnr'].append(psnr)\n",
    "            \n",
    "            psnr_zf = calculate_psnr_single(input, gt, data_range=gt.max())\n",
    "            metrics_dict['psnr_zf'].append(psnr_zf)\n",
    "            \n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "# Validation for one step (plotting)\n",
    "def validate_one_step(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dataloader))  # Load the first batch\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "\n",
    "        # Visualize the first sample in the batch\n",
    "        input_image = torch.abs(torch.complex(x[0, 0, :, :], x[0, 1, :, :])).cpu().numpy()\n",
    "        ground_truth = torch.abs(torch.complex(y[0, 0, :, :], y[0, 1, :, :])).cpu().numpy()\n",
    "        predicted_image = torch.abs(torch.complex(pred[0, 0, :, :], pred[0, 1, :, :])).cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Input\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(\"Input\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Prediction\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(predicted_image, cmap='gray')\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Ground Truth\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(ground_truth, cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "abf76a182d400505",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if is_skip_training:\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    print(f\"Pretrained weight loaded from {pretrained_model_path}\")\n",
    "\n",
    "# Validation\n",
    "metrics_dict = validate(model, test_loader)\n",
    "\n",
    "val_psnr = np.mean(metrics_dict['psnr'])\n",
    "val_psnr_zf = np.mean(metrics_dict['psnr_zf'])\n",
    "\n",
    "print(f\"Validation PSNR: {val_psnr:.4f}\")\n",
    "print(f\"Validation PSNR (ZF): {val_psnr_zf:.4f}\")\n",
    "\n",
    "# Plot\n",
    "validate_one_step(model, test_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4f4e28143a5feb30",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training an GAN-based MRI Reconstruction Model\n",
    "\n",
    "Generative Adversarial Networks (GANs) have been increasingly employed for MRI reconstruction tasks to improve perceptual quality and restore high-frequency details. This section outlines the key components, data handling, and training process for a GAN-based reconstruction model.\n",
    "\n",
    "In a GAN-based MRI reconstruction pipeline, there are two neural networks working in tandem: the generator and the discriminator.\n",
    "\n",
    "- **Generator:** The generator aims to reconstruct fully sampled MRI images from undersampled inputs. It takes undersampled k-space or image domain data as input and outputs a reconstructed image. Common architectures for the generator include U-Net, ResNet, or their variants, designed to handle complex-valued MRI data.\n",
    "\n",
    "- **Discriminator:** The discriminator distinguishes between the reconstructed images (generated by the generator) and the ground truth fully sampled images. It is typically implemented as a convolutional neural network (CNN) that outputs a probability score indicating whether an image is real (ground truth) or fake (generated).\n",
    "\n",
    "The input and target data for training the GAN-based MRI reconstruction model are similar to the end-to-end models:\n",
    "\n",
    "- **Input:** Similar to other MRI reconstruction models, the input consists of undersampled k-space data or image domain representations, often separated into real and imaginary channels for complex-valued data.\n",
    "\n",
    "- **Target:** The ground truth is the fully sampled image or k-space data, also represented in real and imaginary channels.\n",
    "\n",
    "The training objective involves two loss components:\n",
    "\n",
    "- **Reconstruction Loss:** This ensures the generator outputs are close to the ground truth. Common choices include L1 loss, L2 loss, or SSIM-based loss.\n",
    "- **Adversarial Loss:** This encourages the generator to produce outputs indistinguishable from real images, optimized based on feedback from the discriminator.\n",
    "\n",
    "The interplay between the generator and discriminator during training creates a competitive dynamic, helping the generator learn to produce high-quality reconstructed MRI images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3658fc3f7822ed62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete the previous variables\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f43f5678a4bbd047"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "del model, optimizer, train_dataset, test_dataset, train_loader, test_loader, loss_fn\n",
    "del metrics_dict, val_psnr, val_psnr_zf\n",
    "del train_losses_over_epochs, train_losses_over_steps"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b01436882a7e3e6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameters\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0276e1ea8ab8b19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "## Task \n",
    "generator_name = 'unet'\n",
    "# generator_name = 'unet_resi'\n",
    "discriminator_name = 'unet'\n",
    "# discriminator_name = 'vgg'\n",
    "model_name = 'unet_gan'\n",
    "mask_name = 'fMRI_Reg_AF4_CF0.08_PE320'\n",
    "\n",
    "is_debug = False\n",
    "is_save_models = True\n",
    "is_save_all_models = False\n",
    "is_skip_training = False\n",
    "pretrained_generator_path = None\n",
    "# pretrained_generator_path = '/home/jh/simple_mri_recon/runs/unet_gan_fMRI_Reg_AF4_CF0.08_PE320/unet_gan.pth'\n",
    "\n",
    "task_name = '{}_{}'.format(model_name, mask_name) if not is_debug else 'tmp'\n",
    "save_path = os.path.join('runs', task_name)\n",
    "\n",
    "## Dataset\n",
    "train_path = 'dataset/fastmri_tiny/train'\n",
    "test_path = 'dataset/fastmri_tiny/val'\n",
    "train_batch_size = 4\n",
    "test_batch_size = 1\n",
    "\n",
    "## Model\n",
    "in_channels = 2\n",
    "out_channels = 2\n",
    "\n",
    "## Optimisation\n",
    "total_epochs = 100\n",
    "lr_G = 1e-4\n",
    "lr_D = 1e-4\n",
    "\n",
    "## Parameter check\n",
    "assert is_skip_training == (pretrained_generator_path is not None)  # is_skip_training implies pretrained_generator_path is not None\n",
    "assert not is_save_all_models or is_save_models  # is_save_all_models implies is_save_models\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1429a3724bb0e56a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Network\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9020632f4b0e1159"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Discriminator_UNet(nn.Module):\n",
    "    \"\"\"Defines a U-Net discriminator with spectral normalization (SN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc=3, ndf=64):\n",
    "        super(Discriminator_UNet, self).__init__()\n",
    "\n",
    "        norm = spectral_norm\n",
    "\n",
    "        self.conv0 = nn.Conv2d(input_nc, ndf, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv1 = norm(nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False))\n",
    "        self.conv2 = norm(nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False))\n",
    "        self.conv3 = norm(nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False))\n",
    "        # upsample\n",
    "        self.conv4 = norm(nn.Conv2d(ndf * 8, ndf * 4, 3, 1, 1, bias=False))\n",
    "        self.conv5 = norm(nn.Conv2d(ndf * 8, ndf * 2, 3, 1, 1, bias=False))\n",
    "        self.conv6 = norm(nn.Conv2d(ndf * 4, ndf, 3, 1, 1, bias=False))\n",
    "\n",
    "        # extra\n",
    "        self.conv7 = norm(nn.Conv2d(ndf * 2, ndf, 3, 1, 1, bias=False))\n",
    "        self.conv8 = norm(nn.Conv2d(ndf, ndf, 3, 1, 1, bias=False))\n",
    "\n",
    "        self.conv9 = nn.Conv2d(ndf, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = F.leaky_relu(self.conv0(x), negative_slope=0.2, inplace=True)\n",
    "        x1 = F.leaky_relu(self.conv1(x0), negative_slope=0.2, inplace=True)\n",
    "        x2 = F.leaky_relu(self.conv2(x1), negative_slope=0.2, inplace=True)\n",
    "        x3 = F.leaky_relu(self.conv3(x2), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # upsample\n",
    "        x3 = F.interpolate(x3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x4 = F.leaky_relu(self.conv4(x3), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # 256 + 256 --> 512\n",
    "        x4 = torch.cat([x4, x2], dim=1)\n",
    "        x4 = F.interpolate(x4, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x5 = F.leaky_relu(self.conv5(x4), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # 128 + 128 --> 256\n",
    "        x5 = torch.cat([x5, x1], dim=1)\n",
    "        x5 = F.interpolate(x5, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x6 = F.leaky_relu(self.conv6(x5), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # 64 + 64 --> 128\n",
    "        x6 = torch.cat([x6, x0], dim=1)\n",
    "\n",
    "        # extra\n",
    "        out = F.leaky_relu(self.conv7(x6), negative_slope=0.2, inplace=True)\n",
    "        out = F.leaky_relu(self.conv8(out), negative_slope=0.2, inplace=True)\n",
    "        out = self.conv9(out)\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "efc3b643821953e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Network selection\n",
    "def define_generator(model_name):\n",
    "\n",
    "    if model_name == 'unet':\n",
    "        model_type = UNet\n",
    "    elif model_name == 'unet_resi':\n",
    "        model_type = UNet_resi\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model_type\n",
    "\n",
    "\n",
    "# Network selection\n",
    "def define_discriminator(model_name):\n",
    "\n",
    "    if model_name == 'unet':\n",
    "        model_type = Discriminator_UNet\n",
    "    elif model_name == 'vgg':\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model_type"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "83e9aaf3b9a7d066",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the model\n",
    "generator_type = define_generator(generator_name)\n",
    "discriminator_type = define_discriminator(discriminator_name)\n",
    "\n",
    "# Instantiate the model\n",
    "generator = generator_type(in_channels=in_channels, out_channels=out_channels).to(device)\n",
    "discriminator = discriminator_type(input_nc=in_channels, ndf=64).to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "48b45180f5202fe8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Dataloader\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49665d1bed7fe2ed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training and testing dataloaders\n",
    "train_dataset = DatasetFastMRI(data_path_src=train_path, mask_name=mask_name, is_debug=is_debug)\n",
    "test_dataset = DatasetFastMRI(data_path_src=test_path, mask_name=mask_name, is_debug=is_debug)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, drop_last=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "82b22949a0420b7c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Loss Function\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d2aafb1c25a389a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn_recon = partial(total_loss, \n",
    "                        loss_image_weight=15,\n",
    "                        loss_image_type='l1',\n",
    "                        loss_freq_weight=0.1, \n",
    "                        loss_freq_type='l1',\n",
    "                        device=device)\n",
    "\n",
    "loss_fn_adv = nn.BCEWithLogitsLoss().to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "98ba9c7aeff5df9e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Optimisation Setting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8cbfbd45b4ee342"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_G)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_D)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5529dd2db400cd09",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Loop\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d60bb9bfc028078"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 4: Design the Adversarial Loss for GAN Training\n",
    "\n",
    "The adversarial loss is a key component in training GANs, responsible for guiding the generator to produce outputs that the discriminator cannot distinguish from real data. In this exercise, you will design and implement the adversarial loss for GAN-based MRI reconstruction.\n",
    "\n",
    "To create an adversarial loss function that balances the generator and discriminator during GAN training, ensuring realistic and accurate reconstructions.\n",
    "\n",
    "1. **Discriminator Loss:**\n",
    "   The discriminator aims to correctly classify real samples as `real` (label: 1) and generated samples as `fake` (label: 0). The discriminator loss can be formulated as:\n",
    "   \\[\n",
    "   \\mathcal{L}_{D} = -\\mathbb{E}[\\log(D(x))] - \\mathbb{E}[\\log(1 - D(G(z)))]\n",
    "   \\]\n",
    "   where:\n",
    "   - \\(D(x)\\) is the discriminator's output for real images.\n",
    "   - \\(D(G(z))\\) is the discriminator's output for generated images.\n",
    "   - \\(G(z)\\) is the generator's output given input \\(z\\) (undersampled MRI data).\n",
    "\n",
    "2. **Generator Loss:**\n",
    "   The generator aims to fool the discriminator into classifying generated samples as real. The adversarial loss for the generator can be defined as:\n",
    "   \\[\n",
    "   \\mathcal{L}_{G} = -\\mathbb{E}[\\log(D(G(z)))]\n",
    "   \\]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3887a6b261e1ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "def train_gan_one_epoch(generator, discriminator, dataloader, optimizer_G, optimizer_D, loss_fn_recon, loss_fn_adv, epoch, total_epochs, loss_g_weight=1, loss_d_weight=0.1):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    total_loss_G = 0\n",
    "    total_loss_D_real = 0\n",
    "    total_loss_D_fake = 0\n",
    "    all_batch_losses_G = []\n",
    "    all_batch_losses_D_real = []\n",
    "    all_batch_losses_D_fake = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{total_epochs}\", leave=False):\n",
    "        input, gt = batch\n",
    "        input, gt = input.to(device), gt.to(device)\n",
    "        \n",
    "        # Train the generator\n",
    "        \n",
    "        # D: Freeze\n",
    "        for p in discriminator.parameters():\n",
    "            p.requires_grad = False\n",
    "        # G: Zero grad\n",
    "        optimizer_G.zero_grad()\n",
    "        # G: Forward pass\n",
    "        pass  # Add you code...\n",
    "        # D: Forward pass\n",
    "        pass  # Add you code...\n",
    "        # G: Loss\n",
    "        loss_adversarial = ...  # Add you code...\n",
    "        loss_recon = ...  # Add you code...\n",
    "        loss_g_total = loss_g_weight * loss_recon + loss_d_weight * loss_adversarial\n",
    "        # G: Backward pass\n",
    "        loss_g_total.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train the discriminator\n",
    "        \n",
    "        # D: Unfreeze\n",
    "        for p in discriminator.parameters():\n",
    "            p.requires_grad = True\n",
    "        # D: Zero grad\n",
    "        optimizer_D.zero_grad()\n",
    "        # D: Forward pass\n",
    "        pass  # Add you code...  # real data\n",
    "        pass  # Add you code... # fake data, detach to avoid BP to generator\n",
    "        # D: Loss\n",
    "        loss_d_real = ...  # Add you code...  # vs True\n",
    "        loss_d_fake =  ...  # Add you code...  # vs False\n",
    "        # D: Backward pass\n",
    "        loss_d_real.backward()\n",
    "        loss_d_fake.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        total_loss_G += loss_g_total.item()\n",
    "        total_loss_D_real += loss_d_real.item()\n",
    "        total_loss_D_fake += loss_d_fake.item()\n",
    "        all_batch_losses_G.append(loss_g_total.item())\n",
    "        all_batch_losses_D_real.append(loss_d_real.item())\n",
    "        all_batch_losses_D_fake.append(loss_d_fake.item())\n",
    "    return total_loss_G / len(dataloader), total_loss_D_real / len(dataloader), total_loss_D_fake / len(dataloader), all_batch_losses_G, all_batch_losses_D_real, all_batch_losses_D_fake   \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6557e24e97e70afa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Skip training\n",
    "if is_skip_training:\n",
    "    print(\"Skip training\")\n",
    "else:\n",
    "    print(\"Start training\")\n",
    "\n",
    "    train_losses_G_over_epochs = []\n",
    "    train_losses_G_over_steps = []\n",
    "    train_losses_D_real_over_epochs = []\n",
    "    train_losses_D_real_over_steps = []\n",
    "    train_losses_D_fake_over_epochs = []\n",
    "    train_losses_D_fake_over_steps = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(total_epochs):\n",
    "        # print(f\"Epoch {epoch+1}\")\n",
    "        train_loss_G, train_loss_D_real, train_loss_D_fake, batch_losses_G, batch_losses_D_real, batch_losses_D_fake = train_gan_one_epoch(generator, discriminator, train_loader, optimizer_G, optimizer_D, loss_fn_recon, loss_fn_adv, epoch, total_epochs)\n",
    "        train_losses_G_over_epochs.append(train_loss_G)\n",
    "        train_losses_G_over_steps.extend(batch_losses_G)\n",
    "        train_losses_D_real_over_epochs.append(train_loss_D_real)\n",
    "        train_losses_D_real_over_steps.extend(batch_losses_D_real)\n",
    "        train_losses_D_fake_over_epochs.append(train_loss_D_fake)\n",
    "        train_losses_D_fake_over_steps.extend(batch_losses_D_fake)\n",
    "        \n",
    "        # save model\n",
    "        mkdir(save_path)\n",
    "        if is_save_models:\n",
    "            torch.save(generator.state_dict(), os.path.join(save_path, f\"{model_name}.pth\"))\n",
    "            if is_save_all_models:\n",
    "                torch.save(generator.state_dict(), os.path.join(save_path, f\"{model_name}_{epoch+1}.pth\"))\n",
    "    \n",
    "    print(\"Training Completed\")\n",
    "    \n",
    "    # Plot training loss over epochs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_G_over_epochs) + 1), train_losses_G_over_epochs, marker='o')\n",
    "    plt.title(\"Training Loss (Generator) Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_D_real_over_epochs) + 1), train_losses_D_real_over_epochs, marker='o')\n",
    "    plt.title(\"Training Loss (Discriminator - Real) Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_D_fake_over_epochs) + 1), train_losses_D_fake_over_epochs, marker='o')\n",
    "    plt.title(\"Training Loss (Discriminator - Fake) Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training loss over steps\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_G_over_steps) + 1), train_losses_G_over_steps, marker='o', markersize=2)\n",
    "    plt.title(\"Training Loss (Generator) Over Steps\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_D_real_over_steps) + 1), train_losses_D_real_over_steps, marker='o', markersize=2)\n",
    "    plt.title(\"Training Loss (Discriminator - Real) Over Steps\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses_D_fake_over_steps) + 1), train_losses_D_fake_over_steps, marker='o', markersize=2)\n",
    "    plt.title(\"Training Loss (Discriminator - Fake) Over Steps\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9bce1faa1f54bfef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9dcc625c3f57e3f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Validation for testset\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['ssim'] = []\n",
    "    metrics_dict['psnr'] = []\n",
    "    metrics_dict['ssim_zf'] = []\n",
    "    metrics_dict['psnr_zf'] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            input, gt = batch\n",
    "            input, gt = input.to(device), gt.to(device)\n",
    "            pred = model(input)\n",
    "\n",
    "            input = torch.abs(torch.complex(input[:, :1, ...], input[:, 1:, ...]))\n",
    "            pred = torch.abs(torch.complex(pred[:, :1, ...], pred[:, 1:, ...]))\n",
    "            gt = torch.abs(torch.complex(gt[:, :1, ...], gt[:, 1:, ...]))\n",
    "            \n",
    "            input = input.squeeze().float().cpu().numpy()\n",
    "            pred = pred.data.squeeze().float().cpu().numpy()\n",
    "            gt = gt.data.squeeze().float().cpu().numpy()\n",
    "            \n",
    "            ssim = calculate_ssim_single(pred, gt, data_range=gt.max())\n",
    "            psnr = calculate_psnr_single(pred, gt, data_range=gt.max())\n",
    "            metrics_dict['ssim'].append(ssim)\n",
    "            metrics_dict['psnr'].append(psnr)\n",
    "            \n",
    "            ssim_zf = calculate_ssim_single(input, gt, data_range=gt.max())\n",
    "            psnr_zf = calculate_psnr_single(input, gt, data_range=gt.max())\n",
    "            metrics_dict['ssim_zf'].append(ssim_zf)\n",
    "            metrics_dict['psnr_zf'].append(psnr_zf)\n",
    "            \n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "# Validation for one step (plotting)\n",
    "def validate_one_step(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dataloader))  # Load the first batch\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "\n",
    "        # Visualize the first sample in the batch\n",
    "        input_image = torch.abs(torch.complex(x[0, 0, :, :], x[0, 1, :, :])).cpu().numpy()\n",
    "        ground_truth = torch.abs(torch.complex(y[0, 0, :, :], y[0, 1, :, :])).cpu().numpy()\n",
    "        predicted_image = torch.abs(torch.complex(pred[0, 0, :, :], pred[0, 1, :, :])).cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Input\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title(\"Input\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Prediction\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(predicted_image, cmap='gray')\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Ground Truth\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(ground_truth, cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f03ab1cc3d712c39",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if is_skip_training:\n",
    "    generator.load_state_dict(torch.load(pretrained_generator_path))\n",
    "    print(f\"Pretrained weight loaded from {pretrained_generator_path}\")\n",
    "\n",
    "# Validation\n",
    "metrics_dict = validate(generator, test_loader)\n",
    "\n",
    "val_ssim = np.mean(metrics_dict['ssim'])\n",
    "val_psnr = np.mean(metrics_dict['psnr'])\n",
    "val_psnr_zf = np.mean(metrics_dict['psnr_zf'])\n",
    "val_ssim_zf = np.mean(metrics_dict['ssim_zf'])\n",
    "\n",
    "print(f\"Validation SSIM: {val_ssim:.4f}\")\n",
    "print(f\"Validation PSNR: {val_psnr:.4f}\")\n",
    "print(f\"Validation SSIM (ZF): {val_ssim_zf:.4f}\")\n",
    "print(f\"Validation PSNR (ZF): {val_psnr_zf:.4f}\")\n",
    "\n",
    "# Plot\n",
    "validate_one_step(generator, test_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9b8b29e07e8ba1f6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 5: Change the Model to 1 channel Input and Output\n",
    "\n",
    "The model is currently designed to take 2-channel input and output. Modify the model to take 1-channel input and output. Update the model architecture and training pipeline accordingly.\n",
    "\n",
    "See the comparison between the 1-channel and 2-channel models in terms of performance and think about the following questions:\n",
    "- The metrics (PSNR, SSIM, LPIPS) difference?\n",
    "- Which model can persevere the complex-valued information better?\n",
    "- Which model is more suitable for MRI reconstruction?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6121550faefb635"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 6: Change Undersampling Rate\n",
    "\n",
    "The model is currently trained and tested using Cartesain AF4. Change the undersampling rate and re-train the model, see what's different."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c99e57669773243"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b8ddc18307f5d34c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ab346c868489c882"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3eec7b9e14063ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
